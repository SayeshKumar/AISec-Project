{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNWMzKrVuWc"
      },
      "source": [
        "**Revised on 3/5/2024: Changed source files**\n",
        "\n",
        "This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.\n",
        "\n",
        "Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
        "\n",
        "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
        "\n",
        "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "**Step 4:** Download the pre-processed training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUq_FZwmZegw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9998a6b9-a85f-4c62-850e-a677dde41709"
      },
      "source": [
        "# ~8GB\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-06 05:14:32--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.202.129, 3.5.8.176, 3.5.29.195, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.202.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7619200000 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘X_train.dat’\n",
            "\n",
            "X_train.dat         100%[===================>]   7.10G  47.5MB/s    in 2m 23s  \n",
            "\n",
            "2024-03-06 05:16:56 (50.8 MB/s) - ‘X_train.dat’ saved [7619200000/7619200000]\n",
            "\n",
            "--2024-03-06 05:16:56--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.216.217.145, 52.216.78.196, 52.216.152.172, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.216.217.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904800000 (1.8G) [binary/octet-stream]\n",
            "Saving to: ‘X_test.dat’\n",
            "\n",
            "X_test.dat          100%[===================>]   1.77G  49.2MB/s    in 36s     \n",
            "\n",
            "2024-03-06 05:17:32 (50.5 MB/s) - ‘X_test.dat’ saved [1904800000/1904800000]\n",
            "\n",
            "--2024-03-06 05:17:32--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 16.182.68.81, 3.5.8.176, 3.5.7.17, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|16.182.68.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200000 (3.1M) [binary/octet-stream]\n",
            "Saving to: ‘y_train.dat’\n",
            "\n",
            "y_train.dat         100%[===================>]   3.05M  20.2MB/s    in 0.2s    \n",
            "\n",
            "2024-03-06 05:17:33 (20.2 MB/s) - ‘y_train.dat’ saved [3200000/3200000]\n",
            "\n",
            "--2024-03-06 05:17:33--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 16.182.68.81, 3.5.8.176, 3.5.7.17, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|16.182.68.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800000 (781K) [binary/octet-stream]\n",
            "Saving to: ‘y_test.dat’\n",
            "\n",
            "y_test.dat          100%[===================>] 781.25K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-06 05:17:33 (7.63 MB/s) - ‘y_test.dat’ saved [800000/800000]\n",
            "\n",
            "--2024-03-06 05:17:33--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 16.182.68.81, 3.5.8.176, 3.5.7.17, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|16.182.68.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92160330 (88M) [text/csv]\n",
            "Saving to: ‘metadata.csv’\n",
            "\n",
            "metadata.csv        100%[===================>]  87.89M  60.8MB/s    in 1.4s    \n",
            "\n",
            "2024-03-06 05:17:35 (60.8 MB/s) - ‘metadata.csv’ saved [92160330/92160330]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ_JdZKfG7Q-",
        "outputId": "3e1dbbe3-b197-427e-9536-8464d0fc3b3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V958PbDW3H0"
      },
      "source": [
        "**Step 5:** Copy the downloaded files to vMalConv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llip77F3amma"
      },
      "source": [
        "!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRilyqTXnrE"
      },
      "source": [
        "**Step 6:** Download and install Ember:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76bc7PEmlwKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429f7eea-7a9d-44d1-ac47-d1467a6b413d"
      },
      "source": [
        "!pip install git+https://github.com/PFGimenez/ember.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PFGimenez/ember.git\n",
            "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-q5scnzym\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-q5scnzym\n",
            "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ember\n",
            "  Building wheel for ember (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=13050 sha256=ad23fa927f31c132d3175502a3cde261709e3657d6cc19864db88d90928125fa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w0kfar2d/wheels/8f/69/f9/1917c8df03b25fe53e8e2f6cb2c9f61a43dec179b19b10ab9f\n",
            "Successfully built ember\n",
            "Installing collected packages: ember\n",
            "Successfully installed ember-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lief"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRVMSwCQT7D",
        "outputId": "b6fcf0a9-b7d7-4e7f-fea2-5e8a6407adf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lief\n",
            "  Downloading lief-0.14.1-cp310-cp310-manylinux_2_28_x86_64.manylinux_2_27_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lief\n",
            "Successfully installed lief-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXym5qd8Yv8f"
      },
      "source": [
        "**Step 7:** Read vectorized features from the data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfcHyoTsmCFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cdf2c6d-92d0-4882-ca81-08c7aee808fa"
      },
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/vMalConv/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/vMalConv/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTRCz7m7Z7EH"
      },
      "source": [
        "**Step 8:** Get rid of rows with no labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj63lcvin44q",
        "outputId": "3452453c-dd9b-4060-fce8-10c41eac4178",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# labelrows = (y_train != -1)\n",
        "# X_train = X_train[labelrows]\n",
        "# y_train = y_train[labelrows]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.,  0.,  1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVG59AGooyC5",
        "outputId": "7bf8e795-6024-45e3-d37a-e4d26cd9d4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-11955c81bc29>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mh5f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_train.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mh5f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_train.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mdset_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tmUIJNvpZch"
      },
      "source": [
        "!cp /content/X_train.h5 /content/drive/MyDrive/vMalConv/X_train.h5\n",
        "!cp /content/y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."
      ],
      "metadata": {
        "id": "tKoXSzp59RN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code (optional) for sampling the original dataset.\n",
        "import numpy as np\n",
        "\n",
        "samples_per_class = 100000\n",
        "\n",
        "unique_labels = np.unique(y_train[y_train!=-1])\n",
        "\n",
        "selected_indices = []\n",
        "\n",
        "for label in unique_labels:\n",
        "    label_indices = np.where(y_train == label)[0]\n",
        "    random_indices = np.random.choice(label_indices, samples_per_class, replace=True)\n",
        "    selected_indices.extend(random_indices.tolist())\n",
        "\n",
        "X_train = X_train[selected_indices]\n",
        "y_train = y_train[selected_indices]\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "-X9wwv_n9QkY",
        "outputId": "795748b4-9338-4f28-abeb-fdc444df76f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([200000, 2381])\n",
            "torch.Size([200000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bRlBWlaQdd"
      },
      "source": [
        "> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ZlKQwDv4uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90a00a0-6059-4dc7-859f-b440102aa25a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MalConv(nn.Module):\n",
        "    def __init__(self, input_length=2000000, window_size=5, output_dim=1):\n",
        "        super(MalConv, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.input_length = input_length\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=window_size, stride=window_size, bias=True)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=window_size, stride=window_size, bias=True)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=window_size, stride=window_size, bias=True)\n",
        "\n",
        "        # Calculate the output size after the convolutional layers\n",
        "        conv_output_length = 1 +((input_length - window_size) // window_size)\n",
        "        conv_output_length = 1 + ((conv_output_length - window_size) // window_size)\n",
        "        conv_output_length = 1 + ((conv_output_length - window_size) // window_size)\n",
        "        conv_output_length *= 128\n",
        "\n",
        "        self.fc1 = nn.Linear(conv_output_length, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add a channel dimension\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "input_size = 2381\n",
        "\n",
        "# Create a random input tensor with the specified input size\n",
        "random_input = torch.rand(10, input_size)\n",
        "\n",
        "# Instantiate the MalConv model\n",
        "model = MalConv(input_length=input_size)\n",
        "print(model)\n",
        "\n",
        "# Pass the random input through the model\n",
        "output = model(random_input)\n",
        "\n",
        "print(\"Input Size:\", random_input.size())\n",
        "print(\"Output Size:\", output.size())\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MalConv(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(5,))\n",
            "  (conv2): Conv1d(32, 64, kernel_size=(5,), stride=(5,))\n",
            "  (conv3): Conv1d(64, 128, kernel_size=(5,), stride=(5,))\n",
            "  (fc1): Linear(in_features=2432, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Input Size: torch.Size([10, 2381])\n",
            "Output Size: torch.Size([10, 1])\n",
            "tensor([[0.5007],\n",
            "        [0.5011],\n",
            "        [0.5001],\n",
            "        [0.5009],\n",
            "        [0.5005],\n",
            "        [0.5001],\n",
            "        [0.5005],\n",
            "        [0.5012],\n",
            "        [0.5009],\n",
            "        [0.5004]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihnLcFmbaet"
      },
      "source": [
        "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4q5OfK9v9iN"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "mms = StandardScaler()\n",
        "for x in range(0,150000,1000):\n",
        "  mms.partial_fit(X_train[x:x+1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B33Oa1sTxdB0"
      },
      "source": [
        "X_train = mms.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vl5yrex0yY"
      },
      "source": [
        "## Reshape to create 3 channels ##\n",
        "import numpy as np\n",
        "X_train = np.reshape(X_train,(-1,2381))\n",
        "y_train = np.reshape(y_train,(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "a-04XLKNKYPz",
        "outputId": "d55c8a0f-be09-494a-c63e-5e614974d0ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200000, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "YyOvdOYLJPYc",
        "outputId": "892d6a4f-2940-4b9c-d896-371d80c33fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 2381)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."
      ],
      "metadata": {
        "id": "b1iRXFtuvCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming MalConv class definition is already provided as above\n",
        "\n",
        "# Convert your numpy arrays to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDatasets and DataLoaders for training and validation sets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "batch_size = 2048  # Adjust based on your GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Ja3fhJI6qJKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeeb9011-a227-4989-df33-514717b791d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-0e7aa40aa562>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train = torch.tensor(y_train, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMgth6McCqV"
      },
      "source": [
        "> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MalConv model\n",
        "model = MalConv(input_length=2381)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "8_NrI8DSNvGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a712491-12a2-4397-da4b-7cbb23c7004b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MalConv(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(5,))\n",
              "  (conv2): Conv1d(32, 64, kernel_size=(5,), stride=(5,))\n",
              "  (conv3): Conv1d(64, 128, kernel_size=(5,), stride=(5,))\n",
              "  (fc1): Linear(in_features=2432, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "save_dir = \"drive/MyDrive/vMalConv/\"\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 20  # Adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Model checkpoint saved to {checkpoint_path}')\n"
      ],
      "metadata": {
        "id": "iv7piF7dp0lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b193c1b6-d63a-4dfb-8adc-18490c0ddc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.37229777683940113\n",
            "Validation Loss: 0.26051189079880716\n",
            "Epoch 2, Training Loss: 0.2110768337792988\n",
            "Validation Loss: 0.1945260114967823\n",
            "Epoch 3, Training Loss: 0.171075267410731\n",
            "Validation Loss: 0.1829151801764965\n",
            "Epoch 4, Training Loss: 0.1502069597002826\n",
            "Validation Loss: 0.15531195029616357\n",
            "Epoch 5, Training Loss: 0.13120943458774423\n",
            "Validation Loss: 0.13977479115128516\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_5.pt\n",
            "Epoch 6, Training Loss: 0.11649830280979977\n",
            "Validation Loss: 0.1545613318681717\n",
            "Epoch 7, Training Loss: 0.10879426383519475\n",
            "Validation Loss: 0.13119899928569795\n",
            "Epoch 8, Training Loss: 0.0956240020975282\n",
            "Validation Loss: 0.12038677595555783\n",
            "Epoch 9, Training Loss: 0.0889210792470582\n",
            "Validation Loss: 0.11693084761500358\n",
            "Epoch 10, Training Loss: 0.08331964872305907\n",
            "Validation Loss: 0.10859535969793796\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_10.pt\n",
            "Epoch 11, Training Loss: 0.07707774940925309\n",
            "Validation Loss: 0.11097990497946739\n",
            "Epoch 12, Training Loss: 0.07207022239513035\n",
            "Validation Loss: 0.10549777522683143\n",
            "Epoch 13, Training Loss: 0.0665275571938557\n",
            "Validation Loss: 0.09275142699480057\n",
            "Epoch 14, Training Loss: 0.06321946276894098\n",
            "Validation Loss: 0.08817028179764748\n",
            "Epoch 15, Training Loss: 0.058174654747111885\n",
            "Validation Loss: 0.10107610486447811\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_15.pt\n",
            "Epoch 16, Training Loss: 0.05733254471723037\n",
            "Validation Loss: 0.08997605182230473\n",
            "Epoch 17, Training Loss: 0.054701304181089885\n",
            "Validation Loss: 0.08732549995183944\n",
            "Epoch 18, Training Loss: 0.05301504747211179\n",
            "Validation Loss: 0.10041224993765355\n",
            "Epoch 19, Training Loss: 0.04716400179681899\n",
            "Validation Loss: 0.09538422077894211\n",
            "Epoch 20, Training Loss: 0.046139190940162805\n",
            "Validation Loss: 0.09303240813314914\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_20.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3:** Complete the following code to evaluate your trained model on the test data."
      ],
      "metadata": {
        "id": "obToo1WZtD4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Convert test data to PyTorch tensors\n",
        "X_test = mms.transform(X_test[:50000])\n",
        "y_test = y_test[:50000]\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create a TensorDataset and DataLoader for test data\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "aGIiZAS1mYox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e65aa79-3a40-49be-c63a-c3e15965caea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-54b7b7372841>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_test = torch.tensor(y_test, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MalConv(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(5,))\n",
              "  (conv2): Conv1d(32, 64, kernel_size=(5,), stride=(5,))\n",
              "  (conv3): Conv1d(64, 128, kernel_size=(5,), stride=(5,))\n",
              "  (fc1): Linear(in_features=2432, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels_batch in test_loader:\n",
        "\n",
        "        inputs, labels_batch = inputs.to(device), labels_batch.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        # Store predictions and labels\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "precision = precision_score(labels, predictions)\n",
        "recall = recall_score(labels, predictions)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')"
      ],
      "metadata": {
        "id": "83wqvS9jqppe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb41d2c-9c50-49e9-baa3-bcbb0eb65762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5973\n",
            "Precision: 0.5686\n",
            "Recall: 0.8131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4:** Comment on the results in this text box."
      ],
      "metadata": {
        "id": "W6fLYYxps91N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Accuracy (0.5973):\n",
        "\n",
        "This metric indicates the proportion of correctly predicted instances out of the total instances in the test set.\n",
        "An accuracy of approximately 0.5973 suggests that the model is correct in its predictions for roughly 59.73% of the instances in the test set.\n",
        "While accuracy is a useful metric, it may not provide a complete picture of model performance, especially in the presence of class imbalance or misclassification costs.\n",
        "\n",
        "\n",
        "Precision (0.5686):\n",
        "\n",
        "Precision measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
        "A precision score of around 0.5686 indicates that when the model predicts a positive outcome, it is correct approximately 56.86% of the time.\n",
        "Precision is particularly important in tasks where false positives are costly or undesirable, such as medical diagnosis or fraud detection.\n",
        "\n",
        "\n",
        "Recall (0.8131):\n",
        "\n",
        "Recall, also known as sensitivity or true positive rate, measures the proportion of true positive instances that were correctly predicted by the model out of all actual positive instances.\n",
        "A recall score of about 0.8131 suggests that the model correctly identifies approximately 81.31% of the actual positive instances in the dataset.\n",
        "Recall is crucial in scenarios where missing positive instances can have severe consequences, such as in medical screening tests.\n",
        "\n",
        "Overall, the model seems to achieve reasonable performance based on these metrics. However, further analysis, such as considering the trade-offs between precision and recall or examining the model's performance across different classes or subsets of the data, may provide additional insights into its strengths and weaknesses. Additionally, it's essential to interpret these results in the context of the specific problem domain and the requirements of the application."
      ],
      "metadata": {
        "id": "1LQQbl8Uok3A"
      }
    }
  ]
}